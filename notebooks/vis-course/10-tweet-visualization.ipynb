{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from aves.config import setup_style\n",
    "\n",
    "load_dotenv()\n",
    "setup_style()\n",
    "\n",
    "AVES_ROOT = Path(os.environ['AVES_ROOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_PATH = Path(os.environ['TWEET_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_files = sorted(glob(str(TWEET_PATH / '*.gz')))\n",
    "tweet_files[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_json(tweet_files[0], orient='records', lines=True, dtype={'created_at': 'datetime'})\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['user.location'].value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['created_at'].min(), tweets['created_at'].min().round('10min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aves.features.twokenize import tokenize\n",
    "\n",
    "tweets.head()['text'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "\n",
    "def clean_df(df):\n",
    "    return df[\n",
    "        ~df[\"user.location\"].str.contains(\n",
    "            \"Argent|Colom|Per√∫|Ecuador|Bolivia|M√©xico|Mexico|Espa√±a|Cuba|Lima|Dominicana|Costa Rica|Uruguay|Paraguay|El Salvador|Venezuela\",\n",
    "            regex=True, case=False\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "all_tweets = dd.read_json(\n",
    "    tweet_files, orient=\"records\", lines=True, dtype={\"created_at\": \"datetime\"}\n",
    ")[[\"created_at\", \"text\", \"user.location\", \"user.id\", \"user.description\"]].compute()\n",
    "all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(series):\n",
    "    @lru_cache(512)\n",
    "    def cached_tokenize(text):\n",
    "        return tokenize(text)\n",
    "    tokens = series.str.lower().map(cached_tokenize)\n",
    "    counts = Counter()\n",
    "    tokens.map(counts.update)\n",
    "    return counts\n",
    "\n",
    "count_test = all_tweets.head(1000).pipe(clean_df).pipe(lambda x: count_tokens(x['text']))\n",
    "count_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_test.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = all_tweets.pipe(clean_df).pipe(\n",
    "    lambda x: x[x[\"created_at\"] >= \"2022-05-01\"]\n",
    ")\n",
    "len(all_tweets), len(cleaned_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets.resample('10min', on='created_at').size().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"total\", solo miraremos el 33% de los tweets. esto simplifica las cosas pero mantiene los grandes patrones\n",
    "total_count = count_tokens(cleaned_tweets.sample(frac=0.33)['text'])\n",
    "total_count.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aves.visualization.text import draw_wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "draw_wordcloud(ax, total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(AVES_ROOT / 'data' / 'external' / 'stopwords-es.txt') as f:\n",
    "    stopwords = set(f.read().split())\n",
    "\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation + \"‚Äú‚Äù‚Äò‚Äô¬´¬ª¬°¬ø‚Ä¶\"\n",
    "punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cytoolz import keyfilter\n",
    "\n",
    "def filter_tokens(counts):\n",
    "    return keyfilter(lambda x: not x in stopwords and not x in punctuation and not x in ['..', '...'], counts)\n",
    "\n",
    "total_count_filtered = filter_tokens(total_count)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "draw_wordcloud(ax, total_count_filtered)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_count_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(total_count_filtered.items(), columns=['word', 'frequency']).sort_values('frequency', ascending=False)\n",
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words['frequency'].plot(kind='hist', bins=100, logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = set(df_words['word'].values[:1000])\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_top_only(counts):\n",
    "    counts = filter_tokens(counts)\n",
    "    return keyfilter(lambda x: x in top_words, counts)\n",
    "\n",
    "\n",
    "words_x_time = (\n",
    "    cleaned_tweets.sample(frac=0.05)\n",
    "    .resample(\"10min\", on=\"created_at\")\n",
    "    .aggregate(lambda x: keep_top_only(count_tokens(x[\"text\"])))\n",
    "    .apply(pd.Series)\n",
    "    .fillna(0)\n",
    ")\n",
    "words_x_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from aves.features.utils import normalize_rows\n",
    "\n",
    "sns.clustermap(words_x_time.T.pipe(normalize_rows), method='ward', col_cluster=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_x_time['chile'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_x_time['üò°'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_x_time = (\n",
    "    cleaned_tweets\n",
    "    .resample(\"10min\", on=\"created_at\")\n",
    "    .aggregate(lambda x: keep_top_only(count_tokens(x[\"text\"])))\n",
    "    .apply(pd.Series)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "words_x_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_x_time.plot(kind='area', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=8, random_state=666)\n",
    "doc_topics = lda.fit_transform(words_x_time.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aves.features.utils import tfidf\n",
    "\n",
    "word_topics = pd.DataFrame(lda.components_.T, index=words_x_time.columns).pipe(tfidf)\n",
    "word_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    if i < lda.n_components:\n",
    "        draw_wordcloud(ax, word_topics[i])\n",
    "        ax.set_title(f'topic {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(doc_topics, index=words_x_time.index).plot(kind='area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette('cubehelix_r', n_colors=lda.n_components)\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = word_topics.apply(lambda x: '\\n'.join(x.sort_values(ascending=False).head(3).index), axis=0).to_dict()\n",
    "topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aves.visualization.tables.areas import streamgraph\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#fig.set_facecolor(\"#efefef\")\n",
    "#ax.set_facecolor(\"#efefef\")\n",
    "#ax.set_xlim([1920, 2020])\n",
    "#ax.set_ylim([0, 1])\n",
    "#ax.set_title(\"Evoluci√≥n de Nombres en Chile (1920-2020)\", loc=\"left\")\n",
    "#ax.set_ylabel(\"Proporci√≥n de las inscripciones\")\n",
    "#ax.set_xlabel(\"\")\n",
    "\n",
    "streamgraph(\n",
    "    ax,\n",
    "    pd.DataFrame(doc_topics, index=words_x_time.index, columns=topic_labels.values()),\n",
    "    fig=fig,\n",
    "    area_colors=dict(zip(topic_labels.values(), palette)),\n",
    "    baseline=\"wiggle\",\n",
    "    labels=True,\n",
    "    #label_threshold=0.75,\n",
    "    avoid_label_collisions=False,\n",
    "    area_args=dict(linewidth=0.01, alpha=0.75),\n",
    "    label_rolling_window=6\n",
    ")\n",
    "\n",
    "\n",
    "sns.despine(ax=ax, bottom=True, top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_tweets['user.id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = cleaned_tweets.drop_duplicates('user.id', keep='last').pipe(lambda x: x[pd.notnull(x['user.description']) & (x['user.description'].str.len() >= 50)])\n",
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = sentence_model.encode(users['user.description'].values, show_progress_bar=False, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap = UMAP(n_components=3)\n",
    "projected_users = umap.fit_transform(user_embeddings)\n",
    "projected_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_users = pd.DataFrame(projected_users, index=users['user.id'], columns=['x', 'y', 'z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(projected_users['x'], projected_users['y'], projected_users['z'], marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "clustering_model = HDBSCAN(min_cluster_size=100)\n",
    "clusters = pd.Series(clustering_model.fit_predict(projected_users.values), index=projected_users.index, name='cluster')\n",
    "clusters.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "for idx, group in projected_users.join(clusters).groupby('cluster'):\n",
    "    if idx > -1:\n",
    "        ax.scatter(group['x'], group['y'], group['z'], marker='.', label=f'cluster {idx}')\n",
    "    else:\n",
    "        ax.scatter(group['x'], group['y'], group['z'], marker='.', color='grey')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, group in users.join(clusters, on='user.id').groupby('cluster'):\n",
    "    print(idx)\n",
    "    print('\\n'.join(group['user.description'].sample(3).values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aves",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
